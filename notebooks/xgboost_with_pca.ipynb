{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape train: (4209, 378)\n",
      "Shape test: (4209, 377)\n"
     ]
    }
   ],
   "source": [
    "# read datasets\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "# process columns, apply LabelEncoder to categorical features\n",
    "for c in train.columns:\n",
    "    if train[c].dtype == 'object':\n",
    "        lbl = LabelEncoder() \n",
    "        lbl.fit(list(train[c].values) + list(test[c].values)) \n",
    "        train[c] = lbl.transform(list(train[c].values))\n",
    "        test[c] = lbl.transform(list(test[c].values))\n",
    "\n",
    "# shape        \n",
    "print('Shape train: {}\\nShape test: {}'.format(train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    from sklearn.decomposition import PCA, FastICA\n",
    "n_comp = 10\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=n_comp, random_state=42)\n",
    "pca2_results_train = pca.fit_transform(train.drop([\"y\"], axis=1))\n",
    "pca2_results_test = pca.transform(test)\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=42)\n",
    "ica2_results_train = ica.fit_transform(train.drop([\"y\"], axis=1))\n",
    "ica2_results_test = ica.transform(test)\n",
    "\n",
    "# Append decomposition components to datasets\n",
    "for i in range(1, n_comp+1):\n",
    "    train['pca_' + str(i)] = pca2_results_train[:,i-1]\n",
    "    test['pca_' + str(i)] = pca2_results_test[:, i-1]\n",
    "    \n",
    "    train['ica_' + str(i)] = ica2_results_train[:,i-1]\n",
    "    test['ica_' + str(i)] = ica2_results_test[:, i-1]\n",
    "    \n",
    "y_train = train[\"y\"]\n",
    "y_mean = np.mean(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:12.6391\ttest-rmse:12.6385\n",
      "[50]\ttrain-rmse:11.0344\ttest-rmse:11.1705\n",
      "[100]\ttrain-rmse:9.87875\ttest-rmse:10.1732\n",
      "[150]\ttrain-rmse:9.05326\ttest-rmse:9.51649\n",
      "[200]\ttrain-rmse:8.46014\ttest-rmse:9.09636\n",
      "[250]\ttrain-rmse:8.03236\ttest-rmse:8.83399\n",
      "[300]\ttrain-rmse:7.71625\ttest-rmse:8.67202\n",
      "[350]\ttrain-rmse:7.46881\ttest-rmse:8.57553\n",
      "[400]\ttrain-rmse:7.27244\ttest-rmse:8.51903\n",
      "[450]\ttrain-rmse:7.09905\ttest-rmse:8.48881\n",
      "[500]\ttrain-rmse:6.94498\ttest-rmse:8.47294\n",
      "[550]\ttrain-rmse:6.80083\ttest-rmse:8.46735\n",
      "[600]\ttrain-rmse:6.67723\ttest-rmse:8.46464\n",
      "[650]\ttrain-rmse:6.56854\ttest-rmse:8.46462\n",
      "     test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std\n",
      "0         12.638462       0.308924        12.639098        0.154292\n",
      "1         12.603389       0.309063        12.601632        0.153687\n",
      "2         12.568766       0.309246        12.564341        0.154066\n",
      "3         12.534529       0.309728        12.527144        0.153869\n",
      "4         12.500232       0.309396        12.490381        0.153446\n",
      "5         12.466363       0.309787        12.454406        0.153782\n",
      "6         12.433049       0.310534        12.417905        0.153037\n",
      "7         12.399366       0.310665        12.382255        0.153364\n",
      "8         12.366000       0.310830        12.347011        0.153565\n",
      "9         12.333064       0.311205        12.310823        0.153000\n",
      "10        12.300117       0.311722        12.275301        0.152756\n",
      "11        12.267438       0.311395        12.240111        0.152402\n",
      "12        12.235238       0.312018        12.205566        0.152377\n",
      "13        12.203256       0.312252        12.171085        0.152377\n",
      "14        12.171287       0.312052        12.136623        0.152514\n",
      "15        12.140124       0.312264        12.102139        0.152030\n",
      "16        12.108669       0.312507        12.067590        0.151597\n",
      "17        12.077123       0.312925        12.033776        0.151360\n",
      "18        12.046725       0.313379        12.000877        0.151385\n",
      "19        12.015883       0.313929        11.967623        0.151709\n",
      "20        11.984887       0.314003        11.934268        0.151488\n",
      "21        11.954464       0.314136        11.901360        0.151134\n",
      "22        11.924487       0.314264        11.869245        0.151267\n",
      "23        11.894755       0.314327        11.836657        0.150960\n",
      "24        11.864995       0.314112        11.804467        0.151323\n",
      "25        11.835517       0.314317        11.772296        0.151266\n",
      "26        11.806472       0.315072        11.740234        0.151026\n",
      "27        11.777700       0.314849        11.708788        0.150793\n",
      "28        11.748930       0.315455        11.677143        0.150738\n",
      "29        11.720475       0.316108        11.645832        0.150915\n",
      "..              ...            ...              ...             ...\n",
      "577        8.465344       0.402988         6.733741        0.045910\n",
      "578        8.465512       0.402811         6.730901        0.044837\n",
      "579        8.465296       0.402657         6.728614        0.044993\n",
      "580        8.465232       0.402768         6.726153        0.045267\n",
      "581        8.465109       0.402839         6.724311        0.045205\n",
      "582        8.464969       0.402745         6.721517        0.045109\n",
      "583        8.465034       0.402797         6.719057        0.044959\n",
      "584        8.464986       0.402648         6.716679        0.044985\n",
      "585        8.464923       0.402782         6.714711        0.044952\n",
      "586        8.464755       0.402564         6.712214        0.045183\n",
      "587        8.464687       0.402524         6.709097        0.045169\n",
      "588        8.464647       0.402286         6.707055        0.045093\n",
      "589        8.464533       0.402356         6.704427        0.044751\n",
      "590        8.464567       0.402263         6.701833        0.044994\n",
      "591        8.464356       0.402473         6.699925        0.045410\n",
      "592        8.464562       0.402475         6.697232        0.044707\n",
      "593        8.464547       0.402554         6.694650        0.045506\n",
      "594        8.464818       0.402503         6.691694        0.044793\n",
      "595        8.464503       0.402490         6.689399        0.044421\n",
      "596        8.464811       0.402468         6.686615        0.043891\n",
      "597        8.464722       0.402577         6.683599        0.044847\n",
      "598        8.464644       0.402613         6.682166        0.045017\n",
      "599        8.464591       0.402671         6.679720        0.045079\n",
      "600        8.464639       0.402745         6.677233        0.044866\n",
      "601        8.464830       0.402647         6.674537        0.044580\n",
      "602        8.464431       0.402300         6.671908        0.044331\n",
      "603        8.464596       0.402150         6.669315        0.044785\n",
      "604        8.464650       0.402208         6.667079        0.045044\n",
      "605        8.464641       0.402200         6.664631        0.043957\n",
      "606        8.464233       0.402115         6.662465        0.043650\n",
      "\n",
      "[607 rows x 4 columns]\n",
      "607\n"
     ]
    }
   ],
   "source": [
    "()# mmm, xgboost, loved by everyone ^-^\n",
    "import xgboost as xgb\n",
    "\n",
    "# prepare dict of params for xgboost to run with\n",
    "xgb_params = {\n",
    "    'n_trees': 800, \n",
    "    'eta': 0.005,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.80,\n",
    "    'lambda':2,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': y_mean, # base prediction = mean(target)\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "# form DMatrices for Xgboost training\n",
    "dtrain = xgb.DMatrix(train.drop('y', axis=1), y_train)\n",
    "dtest = xgb.DMatrix(test)\n",
    "\n",
    "# xgboost, cross-validation\n",
    "cv_result = xgb.cv(xgb_params, \n",
    "                   dtrain, \n",
    "                   num_boost_round=2000, # increase to have better results (~700)\n",
    "                   early_stopping_rounds=50,\n",
    "                   verbose_eval=50, \n",
    "                   show_stdv=False\n",
    "                  )\n",
    "print cv_result\n",
    "num_boost_rounds = len(cv_result)\n",
    "print(num_boost_rounds)\n",
    "\n",
    "# train model\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.684710294675\n"
     ]
    }
   ],
   "source": [
    "# check f2-score (to get higher score - increase num_boost_round in previous cell)\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# now fixed, correct calculation\n",
    "print(r2_score(dtrain.get_label(), model.predict(dtrain)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make predictions and save results\n",
    "y_pred = model.predict(dtest)\n",
    "output = pd.DataFrame({'id': test['ID'].astype(np.int32), 'y': y_pred})\n",
    "output.to_csv('xgboost-depth{}-pca-ica.csv'.format(xgb_params['max_depth']), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
